# -*- coding: utf-8 -*-
"""NLP Lab 4

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QYbdj-lqcoCE1hhJrugzuET-REB1arFN
"""

print('5')

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import nltk
import sklearn
#from google.colab import files
#uploaded = files.upload() #Добавление файла в проект google colab, используется лишь единожды для занесения файла, тк в дальнейшем будет лишь копирование одного файла

!pip install keybert

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score

from sentence_transformers import SentenceTransformer
from keybert import KeyBERT

from wordcloud import WordCloud
import matplotlib.pyplot as plt

import os
import pandas as pd
import kagglehub

path = kagglehub.dataset_download("snap/amazon-fine-food-reviews")

print(path)

csv_path = os.path.join(path, "Reviews.csv")
df = pd.read_csv(csv_path)

df.head()

df.info()

df_small = df[:10000].copy()

#re.sub(r'<.*?>', '', text)            #html
#re.sub(r'http.\S+|www\.\S+', '', text) #URL
#re.sub(r'\S+@\S+', '', text)          #Email
#re.sub(r'[^а-яё\s]', '', text, flags=re.IGNORECASE) #Оставить только русские буквы и пробельные символы
#re.sub(r'\d+', '', txt) #удаление числительных
#re.sub(r'\d+', 'NUM', text) #Замена числительных на специальный токен
#re.sub(r'\s+', ' ', text).strip()  #Удаление лишних пробелов
#'\n'.join([line.strip() for line in text.split('\n')]) #Удаление пробелов в начале и конце строк

!pip install emoji

import emoji
from nltk.stem import WordNetLemmatizer
import re
from nltk.corpus import stopwords
nltk.download('wordnet')
nltk.download('omw-1.4')
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()
def preprocessing(text):

  text = re.sub(r'<.*?>', '', text)            #html
  text = re.sub(r'http.\S+|www\.\S+', '', text) #URL
  text = re.sub(r'\S+@\S+', '', text)          #Email
  text = emoji.replace_emoji(text, replace='') #Удаление emoji
  text = re.sub(r'[^\w\s]', ' ', text) #Удаление знаков препинания
  text = re.sub(r'\d+', '', text) #удаление числительных

  text = text.lower()
  tokens = text.split()
  tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]
  text = ' '.join(tokens)

  text = re.sub(r'\s+', ' ', text).strip()  #Удаление лишних пробелов
  text = '\n'.join([line.strip() for line in text.split('\n')]) # Удаление пробелов в начале и конце строк

  return text

df_small['Text'] = df_small['Text'].apply(preprocessing)

df_small.head(100)













from sklearn.feature_extraction.text import TfidfVectorizer
tfidf_vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1,2))
tfidf_matrix = tfidf_vectorizer.fit_transform(df_small['Text'])
print(tfidf_matrix.shape)

from sentence_transformers import SentenceTransformer
sbert_model = SentenceTransformer('all-MiniLM-L6-v2')
sbert_embeddings = sbert_model.encode(df_small['Text'].tolist(), show_progress_bar=True)
print(sbert_embeddings.shape)

from sklearn.cluster import KMeans
num_clusters = 5


kmeans_tfidf = KMeans(n_clusters=num_clusters, random_state=42)
clusters_tfidf = kmeans_tfidf.fit_predict(tfidf_matrix)


kmeans_sbert = KMeans(n_clusters=num_clusters, random_state=42)
clusters_sbert = kmeans_sbert.fit_predict(sbert_embeddings)


df_small['cluster_tfidf'] = clusters_tfidf
df_small['cluster_sbert'] = clusters_sbert

print("TF-IDF Silhouette:", silhouette_score(tfidf_matrix, clusters_tfidf))     #Подъодит ли объект своему кластеру (=1 подходит =0 на границе =-1 не в своём кластере)
print("TF-IDF Calinski-Harabasz:", calinski_harabasz_score(tfidf_matrix.toarray(), clusters_tfidf))  #отношение межкластерной дисперсии к внутрикластерной. Большие значения свидетельствуют о компактных и хорошо разделённых кластерах.
print("TF-IDF Davies-Bouldin:", davies_bouldin_score(tfidf_matrix.toarray(), clusters_tfidf))  #среднее сходство каждого кластера с наиболее похожим на него. Чем меньше тем лучше


print("SBERT Silhouette:", silhouette_score(sbert_embeddings, clusters_sbert))  #Подъодит ли объект своему кластеру (=1 подходит =0 на границе =-1 не в своём кластере)
print("SBERT Calinski-Harabasz:", calinski_harabasz_score(sbert_embeddings, clusters_sbert))  #отношение межкластерной дисперсии к внутрикластерной. Большие значения свидетельствуют о компактных и хорошо разделённых кластерах.
print("SBERT Davies-Bouldin:", davies_bouldin_score(sbert_embeddings, clusters_sbert))         #среднее сходство каждого кластера с наиболее похожим на него. Чем меньше тем лучше







tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()

def get_top_words_tfidf(cluster_num, n=10):
    indices = np.where(clusters_tfidf == cluster_num)[0]
    cluster_tfidf_matrix = tfidf_matrix[indices]

    mean_tfidf = np.array(cluster_tfidf_matrix.mean(axis=0)).flatten()
    top_indices = mean_tfidf.argsort()[::-1][:n]
    top_words = [tfidf_feature_names[i] for i in top_indices]
    return top_words

for i in range(num_clusters):
    print(f"TF-IDF Cluster {i}: {get_top_words_tfidf(i)}")



from sentence_transformers import SentenceTransformer
from keybert import KeyBERT

sbert_model = SentenceTransformer('all-MiniLM-L6-v2')
kw_model = KeyBERT(model=sbert_model)

for i in range(num_clusters):
    texts_in_cluster = df_small[df_small['cluster_sbert']==i]['Text'].tolist()[:10000]
    combined_text = " ".join(texts_in_cluster)

    keywords = kw_model.extract_keywords(
        combined_text,
        top_n=10,
        use_mmr=True,
        nr_candidates=20
    )

    print(f"SBERT Cluster {i}: {keywords}")



from wordcloud import WordCloud
import matplotlib.pyplot as plt

def plot_wordcloud(texts, title):
    combined_text = ' '.join(texts)
    wc = WordCloud(width=800, height=400, background_color='white').generate(combined_text)
    plt.figure(figsize=(10,5))
    plt.imshow(wc, interpolation='bilinear')
    plt.axis('off')
    plt.title(title)
    plt.show()

# TF-IDF
for i in range(num_clusters):
    texts = df_small[df_small['cluster_tfidf']==i]['Text'].tolist()           # 0: junk food  1: напитки 2: угощения для домашних животных  3: коффее  4: супермаркет
    plot_wordcloud(texts, f"TF-IDF Cluster {i}")

# SBERT
for i in range(num_clusters):
    texts = df_small[df_small['cluster_sbert']==i]['Text'].tolist()           # 0: продуктовый  1: коффее 2: снеки  3: франшиза  4: напитки
    plot_wordcloud(texts, f"SBERT Cluster {i}")