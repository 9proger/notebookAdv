# -*- coding: utf-8 -*-
"""template.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11wKrSxiMuQI3E0WeIA_fwPazI1KWwbGX

<!-- В VSCode есть специальное расширение для генерации интерактивного оглавления -->
<!-- Желательно в оглавлении сохранить нумерацию пунктов -->
<!-- Правилом хорошего тона является создание интерактивных заголовков пунктов и подпунктов, которые ведут на оглавление -->
**Тема:** Разработка Data Science проекта полного цикла

---

## Оглавление
1. [Предварительная обработка данных](#data-prep)
    * [1.1 Чтение и загрузка данных](#read-load)
    * [1.2 Первичный анализ данных](#data-analys)
    * [1.3 Разделение выборки на обучающую и тестовую выборки](#train-test)
    * [1.4 Обработка вещественных признаков](#data-fill)
    * [1.5 Кодирование категориальных признаков](#data-coder)
    * [1.6 Детекция выбросов и аномалий в данных](#data-anomaly)
    * [1.7 Подведение итогов раздела 1](#data-summary)
2. [Генерация новых признаков](#data-generate)
    * [2.1 Корреляционный анализ входных признаков](#data-heatmap)
    * [2.2 Исправление проблемы мультиколлинеарности в данных](#data-multp)
    * [2.3 Скалирование данных](#data-scaling)
    * [2.4 Подведение итогов раздела 2](#data-summary-2)
3. [Выбор моделей ML и метрик](#data-model)
4. [Обучение моделей ML и подбор гиперпараметров](#data-learning)
5. [Вычисление метрик на новых данных](#data-metrics)
6. [Результат работы](#data-summary-all)

<!-- В этой ячейке необходимо указать, какую задачу вы решаете, в развернутом виде -->
В данной работе решается задача регрессии, где целевым признаком является вещественный признак "Гармония Бессмертия".

<!--
    В этой и последующих ячейках предлагается описать исходные данные, а также сопутствующую информацию:
    1. таблицу с семантикой данных, а также типами данных столбцов (ожидаемый тип, а не фактический, которые мы получаем при чтении данных) в формате MD (кодом не надо выводить таблички)
    2. формулы прикладной области (с указанием, какие формулы можно использовать, а какие -- нет)
-->
| **№** | **Признак** | **Описание** | **Тип данных признака** |
|---|---|---|---|
| **1** | Вектор Мощи | Позиция магического механизма, управляющего интенсивностью перехода через портал. | Вещественный |
| **2** | Скорость перехода через портал | Скорость, с которой портал перемещает объект через пространство. | Вещественный |
| **3** | Приток Силы Потока | Поток магической силы, текущий через ядро портала, обеспечивая его стабильную работу. | Вещественный |
| **4** | Ритм магического ядра | Число оборотов магического ядра портала в минуту. | Вещественный |
| **5** | Поток Энергий | Число оборотов генератора эфира, преобразующего внешнюю энергию в магическую. | Вещественный |
| **6** | Сила Левого Потока | Мощность магического потока, текущего через левую сторону портала, поддерживая его баланс. | Вещественный |
| **7** | Сила Правого Потока | Мощность магического потока, текущего через правую сторону портала, обеспечивая равномерное распределение энергии. | Вещественный |
| **8** | Пламя Стихий | Температура магической энергии, исходящей из высокого магического источника портала, в градусах Цельсия. | Вещественный |
| **9** | Температура вдоха Истока | Температура воздуха, входящего в магический ускоритель портала, в градусах Цельсия. | Целочисленный |
| **10** | Температура выдоха Истока | Температура воздуха, исходящего из магического ускорителя портала, в градусах Цельсия. | Вещественный |
| **11** | Приток давления Выдоха Истока | Давление магического потока на выходе из магического источника высокого давления. | Вещественный |
| **12** | Давление вдоха Истока | Давление воздуха, входящего в магический ускоритель. | Вещественный |
| **13** | Давление выдоха Истока | Давление воздуха, исходящего из магического ускорителя. | Вещественный |
| **14** | Древний Ветер | Давление древней магической энергии, покидающей портал в виде выхлопного потока. | Вещественный |
| **15** | Печать Чародея | Параметр управления магическим впрыском в сердце портала, выраженный в процентах. | Вещественный |
| **16** | Эмульсия Истока | Количество магического топлива, подпитывающего портал. | Вещественный |
| **17** | Дыхание Истока | Коэффициент, отражающий степень угасания магического ускорителя. | Вещественный |
| **18** | Гармония Бессмертия | Коэффициент, указывающий на состояние магического ядра портала и его стабильность, требующий магического восстановления в случае снижения. | Вещественный |
| **19** | Тип Вектора Мощи | Метка типа позиции магического механизма | Категориальный |
| **21** | Номер пометки | Уникальный номер записи о работе портала в блокноте | Целочисленный |

# Общая мощность потока = Сила_Левого_Потока + Сила_Правого_Потока

# Представление о совокупности мощностей = Сила_Левого_Потока + Сила_Правого_Потока + Приток_Силы_Потока

# Общая сила ядра = Ритм_магического_ядра + Приток_Силы_Потока

# Общее давление на выходе = Приток_давления_Выдоха_Истока + Давление_выдоха_Истока

# Магическая производительность = Скорость_перезода_через_портал / Эмульсия_Истока

# Эффективность ядра = Общая_сила_ядра / Эмульсия_Истока

# Для расчёта магической мощности потребуется учесть "Эмульсию Истока", общее давление и разницу между "Пламенем Стихий" и "Температурой вдоха Истока

# Степень износа магических источников = Дыхание_Истока / Гармония_Бессмертия

# Расхождения в стабильности магии = abs(Дыхание_Истока - Гармония_Бессмертия)

# Баланс угасания = (Дыхание_Истока - Гармония_Бессмертия) / Скорость_перехода_через_портал

Данные формулы можно использовать в прикладной области

<!--
    В этой и последующих ячейках предлагается зафиксировать информацию о виртуальном окружении, а именно:
    1. версию питона
    2. версию и список используемых библиотек
    3. основные и часто встречающиеся импорты (pandas, numpy, typing, warnings и т.д.)
    4. случайный сид (seed / random_state)
-->
"""

!pip install pyod

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, KFold, cross_val_score
from sklearn.preprocessing import OneHotEncoder
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer, KNNImputer
from pyod.models.iforest import IForest
from pyod.models.ocsvm import OCSVM
from umap import UMAP
from sklearn.cluster import DBSCAN
import matplotlib.pyplot as plt
from scipy import stats
from scipy.special import kl_div
from scipy.stats import entropy
from matplotlib.axes._axes import Axes
import typing as t
import seaborn as sns
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
#from google.colab import files
#uploaded = files.upload() #Добавление файла в проект google colab, используется лишь единожды для занесения файла, тк в дальнейшем будет лишь копирование одного файла

"""# 1. Предварительная обработка данных <a name="data-prep"></a>

## 1.1. Чтение и загрузка данных <a name="read-load"></a>
"""

pd.set_option('display.max_columns', None) #Для вывода все колонок, тк некоторые в середине становятся "..."

df=pd.read_csv('/content/portal_data.csv', encoding='windows-1251', sep='|') #Чтение файла из google colab (Путь найден через три точки и "копировать путь")
                                                                             #Кодировка и вид разделения данных был выяснен заранее и указан при чтении файла

df.head(10) #Вывод первых 10-ти строк для первичного анализа данных

df.info() #Исследуем признаки с типом данных Object для перевода в числовой тип данных

"""## 1.2. Первичный анализ данных <a name="data-analys"></a>

| **№** | **Признак** | **Описание** | **Тип данных признака** |
|---|---|---|---|
| **1** | Вектор Мощи | Позиция магического механизма, управляющего интенсивностью перехода через портал. | Вещественный |
| **2** | Скорость перехода через портал | Скорость, с которой портал перемещает объект через пространство. | Вещественный |
| **3** | Приток Силы Потока | Поток магической силы, текущий через ядро портала, обеспечивая его стабильную работу. | Вещественный |
| **4** | Ритм магического ядра | Число оборотов магического ядра портала в минуту. | Вещественный |
| **5** | Поток Энергий | Число оборотов генератора эфира, преобразующего внешнюю энергию в магическую. | Вещественный |
| **6** | Сила Левого Потока | Мощность магического потока, текущего через левую сторону портала, поддерживая его баланс. | Вещественный |
| **7** | Сила Правого Потока | Мощность магического потока, текущего через правую сторону портала, обеспечивая равномерное распределение энергии. | Вещественный |
| **8** | Пламя Стихий | Температура магической энергии, исходящей из высокого магического источника портала, в градусах Цельсия. | Вещественный |
| **9** | Температура вдоха Истока | Температура воздуха, входящего в магический ускоритель портала, в градусах Цельсия. | Целочисленный |
| **10** | Температура выдоха Истока | Температура воздуха, исходящего из магического ускорителя портала, в градусах Цельсия. | Вещественный |
| **11** | Приток давления Выдоха Истока | Давление магического потока на выходе из магического источника высокого давления. | Вещественный |
| **12** | Давление вдоха Истока | Давление воздуха, входящего в магический ускоритель. | Вещественный |
| **13** | Давление выдоха Истока | Давление воздуха, исходящего из магического ускорителя. | Вещественный |
| **14** | Древний Ветер | Давление древней магической энергии, покидающей портал в виде выхлопного потока. | Вещественный |
| **15** | Печать Чародея | Параметр управления магическим впрыском в сердце портала, выраженный в процентах. | Вещественный |
| **16** | Эмульсия Истока | Количество магического топлива, подпитывающего портал. | Вещественный |
| **17** | Дыхание Истока | Коэффициент, отражающий степень угасания магического ускорителя. | Вещественный |
| **18** | Гармония Бессмертия | Коэффициент, указывающий на состояние магического ядра портала и его стабильность, требующий магического восстановления в случае снижения. | Вещественный |
| **19** | Тип Вектора Мощи | Метка типа позиции магического механизма | Категориальный |
| **21** | Номер пометки | Уникальный номер записи о работе портала в блокноте | Целочисленный |
"""

df = df.drop('Номер пометки',axis=1) #Признак является идентификатором, поэтому он может быть отброшен

df['Скорость перехода через портал'].value_counts() #Признак имеет пропущенные значения, однако их количество в пределах нормы и можно заменить их медианными значениями

df['Скорость перехода через портал'] = pd.to_numeric(df['Скорость перехода через портал'].replace('-', np.nan))

df['Температура выдоха Истока'].value_counts() #В этом признаке много пропущенный значений, поэтому мы его удаляем

df = df.drop('Температура выдоха Истока',axis=1) #Удаление признака

df['Давление вдоха Истока'].value_counts() #В этом признаке много пропущенный значений, поэтому мы его удаляем

df = df.drop('Давление вдоха Истока',axis=1) #Удаление признака

df['Древний Ветер'].value_counts() #В этом признаке много пропущенный значений, поэтому мы его удаляем

df = df.drop('Древний Ветер',axis=1) #Удаление признака

df['Тип Вектора Мощи'].value_counts() #Приводим категориальный признак к числовому виду

order = ['Слабый', 'Ниже среднего', 'Выше среднего', 'Сильный'] #Индексируем признак в порядке возрастания мощи

vectors = df['Тип Вектора Мощи'].value_counts().reindex(order)
print(vectors)

n=1
for i in vectors.index:
  df['Тип Вектора Мощи']=df['Тип Вектора Мощи'].replace(i,n) #Приводим категориальный признак к числовому виду
  n=n+1
df.head()

df = df.drop('Температура вдоха Истока',axis=1) #Признак имеет лишь одно уникальное значение, поэтому он может быть отброшен

df.info() #Проверка типов данных

"""## 1.3. Разделение выборки на обучающую и тестовую выборки <a name="train-test"></a>"""

seed = 42 #Выставляется сид для случайного разбиения данных на тренировочные и тестовые данные
np.random.seed(seed)

y = df["Гармония Бессмертия"] # целевой признак
X = df.drop(["Гармония Бессмертия"], axis=1)

X_train, X_test, y_train, y_test = train_test_split(
    X,
    y,
    test_size=0.1,   #Обычно я беру 80/20, однако из-за маленького количества данных я решил взять 90/10
    stratify=y,
    shuffle = True,
    random_state=seed
)

"""## 1.4. Обработка вещественных признаков (заполнение пропусков) <a name="data-fill"></a>

Для заполнения пропусков вещественными числами мы будем использовать один из  специальных методов (Knn или Iterative). Чтобы выяснить о лучшем варианте мы самостоятельно удалим и так уже известные нам данные и дадим алгоритмам заполнить их самостоятельно, а потом сравним их между собой и выберем лучший алгоритм для данной задачи
"""

#from sklearn.experimental import enable_iterative_imputer
#from sklearn.metrics import mean_squared_error

def calculate_nrmse(original, imputed, mask):
    original_masked = original[mask]
    imputed_masked = imputed[mask]

    rmse = np.sqrt(mean_squared_error(original_masked, imputed_masked))
    nrmse = rmse / (original.max() - original.min())

    return nrmse

def create_masked_data(data):
    clean_data = data.dropna().copy()


    n_missing = int(len(clean_data) * 0.2)
    missing_indices = np.random.choice(len(clean_data), n_missing, replace=False)

    mask = np.zeros(len(clean_data), dtype=bool)
    mask[missing_indices] = True


    masked_data = clean_data.copy()
    masked_data.iloc[missing_indices] = np.nan

    return clean_data, masked_data, mask


original, masked, mask = create_masked_data(X_train['Скорость перехода через портал'])


X_train_nrsme = X_train.dropna().copy()
X_train_nrsme['Скорость перехода через портал'] = masked

print(f"Всего: {len(original)}")
print(f"Искусственных пропусков: {mask.sum()}")

# iterative импутация
iterative_imputer = IterativeImputer(random_state=seed)
X_train_filled_mrmse_iterative = pd.DataFrame(iterative_imputer.fit_transform(X_train_nrsme), columns=X_train_nrsme.columns)

# KNN импутация
knn_imputer = KNNImputer(n_neighbors=5)
X_train_filled_mrmse_KNN = pd.DataFrame(knn_imputer.fit_transform(X_train_nrsme), columns=X_train_nrsme.columns)

iterative_nrmse = calculate_nrmse(original, X_train_filled_mrmse_iterative['Скорость перехода через портал'], mask)
knn_nrmse = calculate_nrmse(original, X_train_filled_mrmse_KNN['Скорость перехода через портал'], mask)

print(f"iterative: {iterative_nrmse}")
print(f"KNN:  {knn_nrmse}")

"""Исходя из данных было выявлено, что IterativeImputer подходит для этой задачи лучше, поэтому для заполнения вещественных данных будем использовать его"""

#X_train['Скорость перехода через портал'] = pd.to_numeric(X_train['Скорость перехода через портал'].replace('-', np.nan)) #Замена пропусков из "-" в NaN для правильной работы IterativeImputer
iterative_imputer = IterativeImputer(random_state=seed)
X_train = pd.DataFrame(iterative_imputer.fit_transform(X_train), columns=X_train.columns) #Заполнение пропусков IterativeImputer в тренировачных данных
X_test = pd.DataFrame(iterative_imputer.transform(X_test), columns=X_test.columns) # заполнение пропусков IterativeImputer в тестовых данных

"""## 1.5. Кодирование категориальных признаков <a name="data-coder"></a>

Для кодирования категориальных значений был выбран вариант использования OneHotEncoder, тк поле "Тип Вектора Мощи" имеет лишь 4 уникальных значения и может быть разбито на 4 отдельных признака
"""

encoder = OneHotEncoder(sparse_output=False, dtype = 'int')
encoded_array = encoder.fit_transform(X_train[['Тип Вектора Мощи']])

feature_names = [f'Тип Вектора Мощи {int(cat)}' for cat in encoder.categories_[0]]


X_train = X_train.drop('Тип Вектора Мощи', axis=1)
X_train[feature_names] = encoded_array

X_train.head()

X_test_encoded = encoder.transform(X_test[['Тип Вектора Мощи']])

X_test = X_test.drop('Тип Вектора Мощи', axis=1)

X_test[feature_names] = X_test_encoded

X_test.head()

"""## 1.6. Детекция выбросов и аномалий в данных <a name="data-anomaly"></a>"""

from matplotlib.axes._axes import Axes
import typing as t
import seaborn as sns


def draw_sns(df: pd.DataFrame, visualization_function: t.Callable[..., Axes], figsize: t.Tuple[int, int] = (20, 20), **kwargs) -> None:
    """Визуализация диаграмм для каждого столбца датафрейма

    Args:
        df (pd.DataFrame): Набор данных
        visualization_function (t.Callable[..., Axes]): Функция визуализации
        figsize (t.Tuple[int, int], optional): Размер холста. Defaults to (20, 20).
    """
    n_col = 3
    n = len(df.columns)
    n_row = int(np.ceil(n / n_col))

    _, ax = plt.subplots(n_row, n_col, figsize=figsize)

    columns = df.columns

    for i in range(n_row):
        for j in range(n_col):
            k = i * n_col + j
            if k == n:
                break
            column = columns[k]
            x = df[column]
            visualization_function(x, ax=ax[i, j], **kwargs)

draw_sns(X_train, sns.boxplot, figsize=(20, 25))

def calc_bounds(x: t.Iterable[float]) -> t.Tuple[float, float]:
    """Вычисление левой и правой границ IQR

    Args:
        x (t.Iterable[float]): Вариационный ряд

    Returns:
        t.Tuple[float, float]: Левая и правая границы
    """
    q1, q3 = np.percentile(x, [25, 75])
    iqr = q3 - q1
    lower_bound = q1 - 1.5 * iqr
    upper_bound = q3 + 1.5 * iqr
    return lower_bound, upper_bound

def check_feature_outliers_iqr(x: pd.Series) -> np.ndarray[bool]:
    """Проверка выбросов при помощи IQR

    Args:
        x (pd.Series): Вариационный ряд

    Returns:
        np.ndarray[bool]: Массив меток для каждого объекта вариационного ряда, является ли значение выбросом
    """
    lower_bound, upper_bound = calc_bounds(x)
    return ((x < lower_bound) | (x > upper_bound)).values

#from scipy import stats
def check_feature_outliers_robust_zscore(x: pd.Series, coefficient: float = 2.0) -> np.ndarray[bool]:
    """Проверка выбросов при помощи Robust Z-score

    Args:
        x (pd.Series): Вариационный ряд
        coefficient (float): Коэффициент для определения порога (по умолчанию 3.0)

    Returns:
        np.ndarray[bool]: Массив меток для каждого объекта вариационного ряда, является ли значение выбросом
    """
    median = x.median()
    mad = stats.median_abs_deviation(x, scale='normal')  # MAD с нормальным масштабированием

    # Robust Z-score
    robust_z = np.abs(0.6745 * (x - median) / mad)

    return (robust_z > coefficient).values

def check_dataset_outliers(df: pd.DataFrame, outliers_method: t.Callable[..., np.ndarray[bool]], **kwargs) -> np.ndarray[bool]:
    """Проверка выбросов у объектов датасета

    Args:
        df (pd.DataFrame): Датасет
        outliers_method (t.Callable[..., np.ndarray[bool]]): Правило детектирования выбросов

    Returns:
        np.ndarray[bool]: Массив меток для каждого объекта датасета, содержит ли объект выбросы
    """
    outliers_flags = [
        outliers_method(df[column], **kwargs) for column in df.columns
    ]
    return np.any(np.array(outliers_flags), axis=0)

# 1. Находим выбросы методом IQR
outliers_objects_iqr = X_train.loc[check_dataset_outliers(X_train, check_feature_outliers_iqr)]

# 2. Находим выбросы методом Robust Z-score с коэффициентом 2
outliers_objects_z_score_2 = X_train.loc[check_dataset_outliers(X_train, check_feature_outliers_robust_zscore, coefficient=2.0)]

# 3. Находим пересечение методов (IQR и Robust Z-score с коэффициентом 2)
outliers_objects = np.intersect1d(
    outliers_objects_iqr.index,
    outliers_objects_z_score_2.index
)

print(f"Выбросы по IQR: {len(outliers_objects_iqr)} объектов")
print(f"Выбросы по Robust Z-score (коэф. 2): {len(outliers_objects_z_score_2)} объектов")
print(f"Пересечение IQR и Robust Z-score (коэф. 2): {len(outliers_objects)} объектов")

# Выводим общие выбросы
outliers_objects

X_train.index.equals(y_train.index)

common_idx = y_train.index.intersection(outliers_objects)

X_train = X_train.drop(index=common_idx)
y_train = y_train.drop(index=common_idx)

def draw_anomaly(X_2d: np.ndarray[float], anomaly_mask: np.ndarray[bool]) -> None:
    """Визуализация аномальных объектов

    Args:
        X_2d (np.ndarray[float]): Датасет, пониженный до двумерного пространства
        anomaly_mask (np.ndarray[bool]): Маска аномальных объектов
    """
    plt.scatter(X_2d[:, 0], X_2d[:, 1])
    plt.scatter(X_2d[anomaly_mask, 0], X_2d[anomaly_mask, 1], c="red", marker='x')

def draw_anomaly_pie(anomaly_mask: np.ndarray[bool]) -> None:
    """Визуализация долей аномальных и неаномальных объектов в данных

    Args:
        anomaly_mask (np.ndarray[bool]): Маска аномальных объектов
    """
    _, counts = np.unique(anomaly_mask, return_counts=True)

    plt.pie(counts, labels=counts, autopct='%1.0f%%');
    plt.legend([
        'Нормальные объекты', 'Аномалии'
    ], loc='lower right');

X_train

X_train.describe()

X_2d = UMAP().fit_transform(X_train)

isf = IForest().fit(X_train)
isf_anomaly = isf.predict(X_train)

draw_anomaly_pie(isf_anomaly)

draw_anomaly(X_2d, isf_anomaly.astype(bool))

ocsvm = OCSVM().fit(X_train)
ocsvm_anomaly = ocsvm.predict(X_train)

draw_anomaly_pie(ocsvm_anomaly)

draw_anomaly(X_2d, ocsvm_anomaly.astype(bool))

anomalies = np.all(np.array([
    isf_anomaly,
    ocsvm_anomaly,
]), axis=0)

anomaly_index = X_train.loc[anomalies].index

draw_anomaly_pie(anomalies)

draw_anomaly(X_2d, anomalies)

"""После вычисления индексов общих аномальных значений эти строки будут удалены"""

common_idx = y_train.index.intersection(anomaly_index)

X_train = X_train.drop(index=common_idx)
y_train = y_train.drop(index=common_idx)

"""## 1.7. Подведение итогов раздела 1 <a name="data-summary"></a>
<!-- А таких разделах обычно подытоживается вся информация о тех действиях, которые вы делали в разделе -->

В ходе работы было проведено:

анализ данных,

приведение типов к числовому виду,

удаление признаков с большим количеством пропущенных значений,

Заполнение пропусков с помощью IterativeImputer,

Кодирование категориальных признаков,

Удаление выбросов и аномальных значений.

# 2. Генерация новых признаков <a name="data-generate"></a>

## 2.1. Корреляционный анализ входных признаков (построение тепловых карт корреляции) <a name="data-heatmap"></a>
"""

df_heatmap = X_train
df_heatmap_corr = df_heatmap.corr()
sns.heatmap(df_heatmap_corr, annot=True)

"""## 2.2. Исправление проблемы мультиколлинеарности в данных <a name="data-multp"></a>"""

def create_new_features(df):
    """
    Создание новых признаков на основе физических формул из описания задачи
    """
    # Создаем копию датасета
    df_new = df.copy()

    # 1. Общая мощность потоков
    df_new['Представление о совокупности мощностей'] = (
        df['Сила Левого Потока'] +
        df['Сила Правого Потока'] +
        df['Приток Силы Потока']
    )

    # 2. Общая сила ядра (Ритм × Приток Силы)
    df_new['Общая сила ядра'] = (
        df['Ритм магического ядра'] *
        df['Приток Силы Потока']
    )

    # 3. Общее давление на выходе
    df_new['Общее давление на выходе'] = (
        df['Приток давления Выдоха Истока'] +
        df['Давление выдоха Истока']
    )

    # 4. Магическая производительность (Скорость перехода / Эмульсия)
    df_new['Магическая производительность'] = (
        df['Скорость перехода через портал'] /
        df['Эмульсия Истока']
    ).replace([np.inf, -np.inf], 0)

    # 5. Эффективность ядра (Общая сила ядра / Эмульсия)
    df_new['Эффективность ядра'] = (
        df_new['Общая сила ядра'] /
        df['Эмульсия Истока']
    ).replace([np.inf, -np.inf], 0)

    # 7. Магическая мощность (Эмульсия * Общее давление * разница температур)
    df_new['Магическая мощность'] = (
        df['Эмульсия Истока'] *
        df_new['Общее давление на выходе'] *
        (df['Пламя Стихий'] - 736)
    )

    # Удаляем исходные сильно коррелирующие признаки, заменяя их новыми
    # (Оставляем только по одному из каждой коррелирующей пары)
    columns_to_drop = [
        'Сила Левого Потока',
        'Сила Правого Потока',
        'Приток Силы Потока',
        'Ритм магического ядра',
        'Приток давления Выдоха Истока',
        'Давление выдоха Истока',
        'Дыхание Истока'
    ]

    df_new = df_new.drop(columns=columns_to_drop)

    return df_new

# Создаем датасет с новыми признаками
df_transformed = create_new_features(X_train)
X_train = create_new_features(X_train)
X_test = create_new_features(X_test)

"""## 2.3. Скалирование данных <a name="data-scaling"></a>"""

df_transformed.describe()

#from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler

# 1. Стандартизация (StandardScaler)
scaler_standard = StandardScaler()
X_train_standard = scaler_standard.fit_transform(X_train)
X_train_standard = pd.DataFrame(X_train_standard, columns=X_train.columns)

X_test_standard = scaler_standard.transform(X_test)
X_test_standard = pd.DataFrame(X_test_standard, columns=X_test.columns)

# 2. MinMaxScaler
scaler_minmax = MinMaxScaler()
X_train_minmax = scaler_minmax.fit_transform(X_train)
X_train_minmax = pd.DataFrame(X_train_minmax, columns=X_train.columns)

X_test_minmax = scaler_minmax.transform(X_test)
X_test_minmax = pd.DataFrame(X_test_minmax, columns=X_test.columns)


# 3. RobustScaler
scaler_robust = RobustScaler()
X_train_robust = scaler_robust.fit_transform(X_train)
X_train_robust = pd.DataFrame(X_train_robust, columns=X_train.columns)

X_test_robust = scaler_robust.transform(X_test)
X_test_robust = pd.DataFrame(X_test_robust, columns=X_test.columns)

X_train_standard.describe()

X_train_minmax.describe()

X_train_robust.describe()

"""## 2.4. Подведение итогов раздела 2 <a name="data-summary-2"></a>
<!-- А таких разделах обычно подытоживается вся информация о тех действиях, которые вы делали в разделе -->

В ходе работы было приведено:

Исправление мультиколлинеарности, путём замены признаков на формулы,

Скалирование данных

# 3. Выбор моделей ML и метрик <a name="data-model"></a>
"""

features = X_train.columns

for feature in features:
    plt.figure()
    plt.scatter(X_train[feature], y_train)
    plt.xlabel(feature)
    plt.ylabel("Гармония Бессмертия")
    plt.title(f"{feature} vs Гармония Бессмертия")
    plt.show()

data = X_train.copy()
data["Гармония Бессмертия"] = y_train

corr_vector = data.corr()["Гармония Бессмертия"].drop("Гармония Бессмертия")


corr_vector_df = corr_vector.to_frame(name="Гармония Бессмертия")

sns.heatmap(corr_vector_df, annot=True)

"""<!-- Третий раздел самый богатый на писанину. Вам необходимо сначала обосновать выбор моделей, а затем все отобранные модели описать (расписать концептуально, как они работают) -->
Так как перед нами задача регрессии у которой исходя из графиков линейная зависимость, было принято решение воспользоваться моделями линейной регрессии, а именно ElasticNet и Polynomial Regression. Также была выбрана модель SVR, для сравнения результатов с линейными моделями

Модель ElasticNet работает с использованием функции потерь, которая включает штрафы L1 и L2.

L1-регуляризация (Lasso) - уменьшает некоторые коэффициенты регрессии до нуля, что позволяет отбирать признаки.
L2-регуляризация (Ridge) - уменьшает коэффициенты к нулю, но не устанавливает их точно к нулю.

Polynomial Regression работает, моделируя нелинейные зависимости в данных. Это расширение линейной регрессии, где вместо линейного уравнения используется уравнение двух и более степеней.

Принцип работы SVR заключается в нахождении гиперплоскости в многомерном пространстве признаков, которая наилучшим образом соответствует обучающим данным и минимизирует ошибку прогнозирования для задач регрессии

<!-- И снова писанина: необходимо не только описать, какие метрики вы используете, но привести формулы их вычисления -->
Для своей работы я выбрал метрики "MAE","MSE","RMSE","MAPE","R^2"

$$
MAE = \frac{1}{n} \sum_{i=1}^{n} \left| y_i - \hat{y}_i \right|
$$



$$
MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
$$



$$
RMSE = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2}
$$



$$
MAPE = \frac{1}{n} \sum_{i=1}^{n}
\left| \frac{y_i - \hat{y}_i}{y_i} \right|
$$



$$
R^2 = 1 -
\frac{\sum_{i=1}^{n} (y_i - \hat{y}_i)^2}
{\sum_{i=1}^{n} (y_i - \bar{y})^2}
$$
​
$$- y_i - истинное значение целевой переменной  $$
$$- \hat{y}_i - предсказанное значение $$
$$- n - количество наблюдений $$
"""

# импорт моделей из пула
from sklearn.linear_model import LinearRegression, ElasticNet
from sklearn.preprocessing import PolynomialFeatures
from sklearn.ensemble import IsolationForest, RandomForestRegressor
from sklearn.pipeline import Pipeline
from sklearn import svm

# формирование сеток гиперпараметров
parameters_Elastic = {
    "alpha": ("float", 1e-2, 10, True),
    "l1_ratio": ("float", 0.0, 1.0, False)
}
parameters_SVR = {
    "C": ("float", 1e-3, 1e2, True),
    "epsilon": ("float", 1e-3, 0.5, False),
    "gamma": ("categorical", ["scale", "auto"])
}

parameters_Forest = {
    "n_estimators": ("int", 10, 200),            # Количество деревьев
    "max_depth": ("int", 3, 20),                 # Максимальная глубина
}

# импорт метрик из пула
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, mean_absolute_percentage_error

"""# 4. Обучение моделей ML и подбор гиперпараметров <a name="data-learning"></a>"""

pip install optuna

import optuna

# подбор гиперпараметров и обучения лучших моделей
import optuna
def cross(model,X_train,y_train):
  # Инициализация KFold
  kf = KFold(n_splits=5, shuffle=True, random_state=42)
  scores = cross_val_score(model, X_train, y_train, cv=kf, scoring='neg_mean_squared_error')
  return scores

def Optun(model, X_train, y_train, X_test, y_test, parameters):

    def objective(trial):

        params = {}

        for param_name, param_info in parameters.items():

            param_type = param_info[0]

            if param_type == "float":
                _, low, high, log = param_info
                params[param_name] = trial.suggest_float(
                    param_name, low, high, log=log
                )

            elif param_type == "int":
                _, low, high = param_info
                params[param_name] = trial.suggest_int(
                    param_name, low, high
                )

            elif param_type == "categorical":
                _, choices = param_info
                params[param_name] = trial.suggest_categorical(
                    param_name, choices
                )

            else:
                raise ValueError(f"Unknown parameter type: {param_type}")

        model_ = model.set_params(**params)

        model_.fit(X_train, y_train)
        y_pred = model_.predict(X_test)

        mse = mean_squared_error(y_test, y_pred) #MSE является гладкой и дифференцируемой функцией, что делает её удобной для оптимизации
        return mse

    study = optuna.create_study(direction="minimize")
    study.optimize(objective, n_trials=100)

    return study.best_params

model = ElasticNet()
OptunaP = Optun(model,X_train,y_train,X_test,y_test,parameters_Elastic)
OptunaScores = cross(model.set_params(**OptunaP),X_train,y_train)

print(OptunaP,'ElasticNet')
print(OptunaScores,'ElasticNet')

model = svm.SVR()
OptunaP = Optun(model,X_train,y_train,X_test,y_test,parameters_SVR)
OptunaScores = cross(model.set_params(**OptunaP),X_train,y_train)

print(OptunaP,'SVR')
print(OptunaScores,'SVR')

#model = IsolationForest()
#OptunaP = Optun(model,X_train,y_train,X_test,y_test,parameters_Forest)
#OptunaScores = cross(model.set_params(**OptunaP),X_train,y_train)

#print(OptunaP,'IsolationForest')
#print(OptunaScores,'IsolationForest')

model = RandomForestRegressor()
OptunaP = Optun(model, X_train, y_train, X_test, y_test, parameters_Forest)
OptunaScores = cross(model.set_params(**OptunaP), X_train, y_train)

print(OptunaP,'RandomForestRegressor')
print(OptunaScores,'RandomForestRegressor')

Input = [('standardscaler', StandardScaler()), ('polynomial', PolynomialFeatures(degree=2, include_bias=False)), ('model', LinearRegression()) ]
pipe = Pipeline(Input)
pipe.fit(X_train, y_train)
Yhat_pipe = pipe.predict(X_test)

"""# 5. Вычисление метрик на новых данных <a name="data-metrics"></a>"""

sets = [('',X_train, X_test), ('standart',X_train_standard, X_test_standard),  ('minmax',X_train_minmax, X_test_minmax), ('robust',X_train_robust, X_test_robust)]

# ячейки для вычисления метрик на лучших моделях и их сопоставление
from math import sqrt
Headings1=["Регрессор","MAE","MSE","RMSE","MAPE","R^2","MAE","MSE","RMSE","MAPE","R^2"]

table1=[]


classifiers = {
    'ElasticNet': ElasticNet(alpha = 0.06, l1_ratio = 0.0006),
    'SVR': svm.SVR(C = 0.44, epsilon = 0.0015, gamma = 'scale'),
    'RandomForestRegressor': RandomForestRegressor(n_estimators = 10, max_depth=3)
}

for index, (name, classifier) in enumerate(classifiers.items()):
    for scale, X_train, X_test in sets:


      classifier.fit(X_train, np.ravel(y_train))

      y_pred = classifier.predict(X_train)
      a=([round(mean_absolute_error(y_train, y_pred),4),round(mean_squared_error(y_train, y_pred),4),round(sqrt(mean_squared_error(y_train, y_pred)),4),round(sqrt(mean_absolute_percentage_error(y_train, y_pred)),4),round(r2_score(y_train, y_pred),2)])

      y_pred = classifier.predict(X_test)
      b=([round(mean_absolute_error(y_test, y_pred),4),round(mean_squared_error(y_test, y_pred),4),round(sqrt(mean_squared_error(y_test, y_pred)),4),round(sqrt(mean_absolute_percentage_error(y_test, y_pred)),4),round(r2_score(y_test, y_pred),2)])


      y_test_np=np.array(y_test)



      table1.append([name + ' ' +scale]+a+b)

for scale, X_train, X_test in sets:

  y_Polinom_pred = pipe.predict(X_train)
  a=([round(mean_absolute_error(y_train, y_Polinom_pred),4),round(mean_squared_error(y_train, y_Polinom_pred),4),round(sqrt(mean_squared_error(y_train, y_Polinom_pred)),4),round(sqrt(mean_absolute_percentage_error(y_train, y_Polinom_pred)),4),round(r2_score(y_train, y_Polinom_pred),2)])
  y_Polinom_pred = pipe.predict(X_test)
  b=([round(mean_absolute_error(y_test, y_Polinom_pred),4),round(mean_squared_error(y_test, y_Polinom_pred),4),round(sqrt(mean_squared_error(y_test, y_Polinom_pred)),4),round(sqrt(mean_absolute_percentage_error(y_test, y_Polinom_pred)),4),round(r2_score(y_test, y_Polinom_pred),2)])


  table1.append(["Polinomial Regression" +''+ scale]+a+b)

table_Scikit_Custom=pd.DataFrame(table1, columns=Headings1)
table_Scikit_Custom.head(20)

"""# 6. Результат работы <a name="data-summary-all"></a>

<!-- Тут обычно пишется, какая модель показала себя лучше, с какими метриками и рекомендуется ли ее выводить в прод -->
Исходя из данных, можно сказать, что лучше всего себя показала ElasticNet, тк метрики для тренировочной и тестовой выборки одинаковые и низкие, что говорит о том, что модель не переобучена. И показатель R^2 показывает сравнительно лучший результат. Однако в общем случае, эту модель нельзя использовать, так как показатели всё равно катастрофически плохи и модель не подходит для практического использования.
"""